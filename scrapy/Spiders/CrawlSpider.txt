class scrapy.spiders.CrawlSpider
这是爬取常规网站最常用的爬虫。它通过定义一系列规则为追踪链接提供了便捷的机制。它对于一般场合来说已足够通用，因此你可以从它开始并为更功能性定制的需求覆盖他，又抑或是实现你自己的爬虫。
除了从 Spider 继承来的属性，这个类还支持一个新的属性：
rules：
	Rule 对象列表。每个 Rule 为爬取站点定义特定行为。如果多个 rules 匹配到相同的链接，那么第一个会被使用，根据它们在属性中定义的顺序。

parse_start_url(response)
	这个方法被 start_urls response 调用。它允许解析初始 responses 且必须返回 一个 Item 对象 或 Request 对象 或包含它们任一个的可迭代内容。

Crawling rules：
class scrapy.spiders.Rule(link_extractor=None, callback=None, cb_kwargs=None, follow=None, process_links=None, process_request=None, errback=None)

link_extractor 是一个 Link Extractor 对象，它定义了链接是如何从每个被爬取网页提取的。每个产生的链接都会被用于生成一个 Request 对象，这个对象包含处于 meta 字典中的链接文本（即 link_text key对应的内容）。如果被遗漏了，一个没有变量的默认链接提取器将会被使用，导致所有链接都会被提取。
callback 是一个可调用的或一个字符串，由使用指定链接提取器提取的链接调用。callback 接受一个 Response 作为它的第一个变量且必须返回一个单实例或 Item 、dict 和 Request 对象之一的可迭代。就像上面提到的，接收到的Response 对象将会包含链接文本。
cb_kwargs 是一个字典，它包含传给 callback 函数的 keyword 变量。
follow 是一个布尔值，它指定是否链接应该被每个使用这条规则提取的response 跟踪。

后面的用到再翻译
process_links is a callable, or a string (in which case a method from the spider object with that name will be used) which will be called for each list of links extracted from each response using the specified link_extractor. This is mainly used for filtering purposes.

process_request is a callable (or a string, in which case a method from the spider object with that name will be used) which will be called for every Request extracted by this rule. This callable should take said request as first argument and the Response from which the request originated as second argument. It must return a Request object or None (to filter out the request).

errback is a callable or a string (in which case a method from the spider object with that name will be used) to be called if any exception is raised while processing a request generated by the rule. It receives a Twisted Failure instance as first parameter.